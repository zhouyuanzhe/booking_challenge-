{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BOOKING_challenge_code_team_dasou.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1fa026a61c844c4a93d010c9bde302b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8f42e827ed641909fb89abb1b2b7ba2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f1b98b8d4bd4b77b020bc28dce19dbc",
              "IPY_MODEL_7b4c1103e511446aa61d2bf23cd81dc1"
            ]
          }
        },
        "d8f42e827ed641909fb89abb1b2b7ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f1b98b8d4bd4b77b020bc28dce19dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8b789b42b84c4f49b8437d7e0741376f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70662,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70662,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ff944d0ce084b4d9f5f0c03b177dd1e"
          }
        },
        "7b4c1103e511446aa61d2bf23cd81dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed5b78921aa3459098367fdfccadc40f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70662/70662 [00:01&lt;00:00, 38213.66it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a567f2172d254ef49b6b2bddb93ffe98"
          }
        },
        "8b789b42b84c4f49b8437d7e0741376f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ff944d0ce084b4d9f5f0c03b177dd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed5b78921aa3459098367fdfccadc40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a567f2172d254ef49b6b2bddb93ffe98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwAProppzf5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137e54af-e709-4421-8021-60a2b626f7a2"
      },
      "source": [
        "import os\r\n",
        "if not os.path.exists('bookingchallenge.zip'):\r\n",
        "  !wget https://shoutout.wix.com/so/e7NOQRe3s/c?w=Yf6RfAsxuTZ-iwMp4ctugpiHMNm6tqV23NoKlVp3LI4.eyJ1IjoiaHR0cHM6Ly9tZWRpYS53aXguY29tL2FyY2hpdmVzLzMwOTFiZV85ZjI0NWM3ZjY0YWY0ZWQyODMxOGE3ZWI5MGNlMjk0Yy56aXAiLCJyIjoiZTE2YjIwZjItZjE5Zi00MDRmLTAzMDYtYjRlYTFlZjM2NjgzIiwibSI6Im1haWwiLCJjIjoiYzYzZDdjNGYtNzMzMy00ZGM2LWJlMWItODdiZTc5NjQ3NThiIn0 -O bookingchallenge.zip\r\n",
        "  !unzip bookingchallenge.zip\r\n",
        "  !wget https://shoutout.wix.com/so/c4NRCVgeV/c?w=P0QHVGbLhSfHoFBEanWqgTNMNDnH2kFiVG8z8jnuL6U.eyJ1IjoiaHR0cHM6Ly8wMzUxMDVmNy1hZTMyLTQ3YjYtYTI1Yi04N2FmNzkyNGM3ZWEudXNyZmlsZXMuY29tL2FyY2hpdmVzLzMwOTFiZV84Mzc4NGYzMzA1NWM0NzkxYWUyYjY4ODdlMTFkZGYwOS56aXA_ZG49QktOR3Rlc3RkYXRhV1NETTIwMjEuemlwIiwiciI6IjAzMzQzYjU3LTQ5MGYtNDk3OS02NGFjLTZjZTJlYjBiZDU0NyIsIm0iOiJtYWlsIiwiYyI6ImM2M2Q3YzRmLTczMzMtNGRjNi1iZTFiLTg3YmU3OTY0NzU4YiJ9 -O testdata.zip\r\n",
        "  !unzip -n testdata.zip  \r\n",
        "  !pip install adabelief-tf==0.2.0\r\n",
        "\r\n",
        "if not os.path.exists('./drive/My Drive/predictions/'):\r\n",
        "  !mkdir ./drive/My\\ Drive/predictions/\r\n",
        "\r\n",
        "import time\r\n",
        "T = int(time.time())\r\n",
        "path = './drive/My Drive/predictions/sub_%s/'%T\r\n",
        "os.mkdir(path)\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.metrics import sparse_top_k_categorical_accuracy\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "from gensim.models import Word2Vec, FastText\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "\r\n",
        "from sklearn import preprocessing\r\n",
        "from tensorflow.keras.layers import Embedding\r\n",
        "import sys\r\n",
        "import warnings\r\n",
        "warnings.simplefilter(\"ignore\")\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM, Dropout, GlobalAveragePooling1D, Conv1D, MaxPooling1D, add, BatchNormalization, Lambda\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from tensorflow.keras import initializers, regularizers, constraints\r\n",
        "from tensorflow.keras.layers import Layer\r\n",
        "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\r\n",
        "from gensim.models import Word2Vec\r\n",
        "\r\n",
        "from adabelief_tf import AdaBeliefOptimizer\r\n",
        "from sklearn.model_selection import KFold\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-22 17:16:24--  https://shoutout.wix.com/so/e7NOQRe3s/c?w=Yf6RfAsxuTZ-iwMp4ctugpiHMNm6tqV23NoKlVp3LI4.eyJ1IjoiaHR0cHM6Ly9tZWRpYS53aXguY29tL2FyY2hpdmVzLzMwOTFiZV85ZjI0NWM3ZjY0YWY0ZWQyODMxOGE3ZWI5MGNlMjk0Yy56aXAiLCJyIjoiZTE2YjIwZjItZjE5Zi00MDRmLTAzMDYtYjRlYTFlZjM2NjgzIiwibSI6Im1haWwiLCJjIjoiYzYzZDdjNGYtNzMzMy00ZGM2LWJlMWItODdiZTc5NjQ3NThiIn0\n",
            "Resolving shoutout.wix.com (shoutout.wix.com)... 185.230.60.168\n",
            "Connecting to shoutout.wix.com (shoutout.wix.com)|185.230.60.168|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.wix.com/archives/3091be_9f245c7f64af4ed28318a7eb90ce294c.zip [following]\n",
            "--2021-01-22 17:16:25--  https://media.wix.com/archives/3091be_9f245c7f64af4ed28318a7eb90ce294c.zip\n",
            "Resolving media.wix.com (media.wix.com)... 34.102.176.152\n",
            "Connecting to media.wix.com (media.wix.com)|34.102.176.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16698079 (16M) [application/zip]\n",
            "Saving to: ‘bookingchallenge.zip’\n",
            "\n",
            "bookingchallenge.zi 100%[===================>]  15.92M  13.9MB/s    in 1.1s    \n",
            "\n",
            "2021-01-22 17:16:26 (13.9 MB/s) - ‘bookingchallenge.zip’ saved [16698079/16698079]\n",
            "\n",
            "Archive:  bookingchallenge.zip\n",
            "  inflating: booking_train_set.csv   \n",
            "  inflating: __MACOSX/._booking_train_set.csv  \n",
            "  inflating: Readme.md               \n",
            "  inflating: __MACOSX/._Readme.md    \n",
            "--2021-01-22 17:16:27--  https://shoutout.wix.com/so/c4NRCVgeV/c?w=P0QHVGbLhSfHoFBEanWqgTNMNDnH2kFiVG8z8jnuL6U.eyJ1IjoiaHR0cHM6Ly8wMzUxMDVmNy1hZTMyLTQ3YjYtYTI1Yi04N2FmNzkyNGM3ZWEudXNyZmlsZXMuY29tL2FyY2hpdmVzLzMwOTFiZV84Mzc4NGYzMzA1NWM0NzkxYWUyYjY4ODdlMTFkZGYwOS56aXA_ZG49QktOR3Rlc3RkYXRhV1NETTIwMjEuemlwIiwiciI6IjAzMzQzYjU3LTQ5MGYtNDk3OS02NGFjLTZjZTJlYjBiZDU0NyIsIm0iOiJtYWlsIiwiYyI6ImM2M2Q3YzRmLTczMzMtNGRjNi1iZTFiLTg3YmU3OTY0NzU4YiJ9\n",
            "Resolving shoutout.wix.com (shoutout.wix.com)... 185.230.60.168\n",
            "Connecting to shoutout.wix.com (shoutout.wix.com)|185.230.60.168|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://035105f7-ae32-47b6-a25b-87af7924c7ea.usrfiles.com/archives/3091be_83784f33055c4791ae2b6887e11ddf09.zip?dn=BKNGtestdataWSDM2021.zip [following]\n",
            "--2021-01-22 17:16:28--  https://035105f7-ae32-47b6-a25b-87af7924c7ea.usrfiles.com/archives/3091be_83784f33055c4791ae2b6887e11ddf09.zip?dn=BKNGtestdataWSDM2021.zip\n",
            "Resolving 035105f7-ae32-47b6-a25b-87af7924c7ea.usrfiles.com (035105f7-ae32-47b6-a25b-87af7924c7ea.usrfiles.com)... 34.102.176.152\n",
            "Connecting to 035105f7-ae32-47b6-a25b-87af7924c7ea.usrfiles.com (035105f7-ae32-47b6-a25b-87af7924c7ea.usrfiles.com)|34.102.176.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21931123 (21M) [application/zip]\n",
            "Saving to: ‘testdata.zip’\n",
            "\n",
            "testdata.zip        100%[===================>]  20.92M  7.58MB/s    in 2.8s    \n",
            "\n",
            "2021-01-22 17:16:31 (7.58 MB/s) - ‘testdata.zip’ saved [21931123/21931123]\n",
            "\n",
            "Archive:  testdata.zip\n",
            "  inflating: booking_test_set.csv    \n",
            "  inflating: evaluation_sample.ipynb  \n",
            "  inflating: sample_test_set.csv     \n",
            "  inflating: __MACOSX/._sample_test_set.csv  \n",
            "  inflating: sample_truth.csv        \n",
            "  inflating: __MACOSX/._sample_truth.csv  \n",
            "  inflating: submission.csv          \n",
            "  inflating: __MACOSX/._submission.csv  \n",
            "Collecting adabelief-tf==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/8a/aaa9616dba0e316d1fda3a3167ba0884a542feab7aa8e04bdcb53f8e71b8/adabelief_tf-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from adabelief-tf==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.6/dist-packages (from adabelief-tf==0.2.0) (0.8.7)\n",
            "Collecting colorama>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.3.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.12.4)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.10.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.1.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (51.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.3.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (4.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.4.0)\n",
            "Installing collected packages: colorama, adabelief-tf\n",
            "Successfully installed adabelief-tf-0.2.0 colorama-0.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIidGmSozh0G"
      },
      "source": [
        "data = pd.read_csv('./booking_train_set.csv')\r\n",
        "data_test = pd.read_csv('./booking_test_set.csv')\r\n",
        "data_test = data_test.sort_values(by=['utrip_id','checkin'])\r\n",
        "\r\n",
        "N_city = data.city_id.nunique()\r\n",
        "N_hotel = data.hotel_country.nunique()\r\n",
        "N_affi = data.affiliate_id.nunique()\r\n",
        "\r\n",
        "# 改变城市id的范围\r\n",
        "le = preprocessing.LabelEncoder()\r\n",
        "data['city_id'] = le.fit_transform(data['city_id'].values)\r\n",
        "data['city_id'] = data['city_id'].astype(str)\r\n",
        "\r\n",
        "le_hotel = preprocessing.LabelEncoder()\r\n",
        "data['hotel_country'] = le_hotel.fit_transform(data['hotel_country'].values)\r\n",
        "data['hotel_country'] = data['hotel_country'].astype(str)\r\n",
        "\r\n",
        "# 去除长度为1的序列\r\n",
        "data['Size'] = data.utrip_id.map(data.groupby('utrip_id').agg('size'))\r\n",
        "data = data[data.Size>1]\r\n",
        "\r\n",
        "data['time'] = data.checkin.apply(lambda x: int(x[:4]))\r\n",
        "data = data[data.time!=2015] # 去掉时间上的outlier\r\n",
        "\r\n",
        "data_test['city_id'] = data_test.city_id.apply(lambda x: 56430 if x==0 else x)\r\n",
        "data_test = data_test.fillna('Urkesh')\r\n",
        "data_test['Size'] = data_test.utrip_id.map(data_test.groupby('utrip_id').agg('size'))\r\n",
        "\r\n",
        "data_test['city_id'] = le.transform(data_test['city_id'].values)\r\n",
        "data_test['city_id'] = data_test['city_id'].astype(str)\r\n",
        "\r\n",
        "data_test['hotel_country'] = le_hotel.transform(data_test['hotel_country'].values)\r\n",
        "data_test['hotel_country'] = data_test['hotel_country'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocmJNCrxzhvb"
      },
      "source": [
        "N = data.user_id.nunique()\r\n",
        "\r\n",
        "data_train = data     # Train dataset\r\n",
        "data_test = data_test   # Test dataset\r\n",
        "\r\n",
        "def trans_sincos(df, name='week', MAX=7):\r\n",
        "  df[name+'_sin'] = np.sin(np.pi*df[name].values/MAX)\r\n",
        "  df[name+'_cos'] = np.cos(np.pi*df[name].values/MAX)\r\n",
        "  return df \r\n",
        "\r\n",
        "def extract_time(df):\r\n",
        "  df['dayofweek_in'] = pd.to_datetime(df.checkin).dt.dayofweek.values\r\n",
        "  df['dayofmonth_in'] = pd.to_datetime(df.checkin).dt.day.values\r\n",
        "  df['dayofweek_out'] = pd.to_datetime(df.checkout).dt.dayofweek.values\r\n",
        "  df['dayofmonth_out'] = pd.to_datetime(df.checkout).dt.day.values\r\n",
        "  df['month_in'] = pd.to_datetime(df.checkin).dt.month.values\r\n",
        "  df['month_out'] = pd.to_datetime(df.checkout).dt.month.values\r\n",
        "\r\n",
        "  df = trans_sincos(df, name='dayofweek_in', MAX=7)\r\n",
        "  df = trans_sincos(df, name='dayofmonth_in', MAX=30)\r\n",
        "  df = trans_sincos(df, name='dayofweek_out', MAX=7)\r\n",
        "  df = trans_sincos(df, name='dayofmonth_out', MAX=30)\r\n",
        "  df = trans_sincos(df, name='month_in', MAX=12)\r\n",
        "  df = trans_sincos(df, name='month_out', MAX=12)\r\n",
        "\r\n",
        "  df['duration'] = (pd.to_datetime(df.checkout)-pd.to_datetime(df.checkin)).dt.days.values\r\n",
        "  df[df.duration>19]['duration']=19\r\n",
        "\r\n",
        "  return df\r\n",
        "\r\n",
        "def get_additionnal_features(data_train, data_test):\r\n",
        "  feat_names = ['duration',  'month_in_sin', 'month_out_sin', 'month_in_cos', 'month_out_cos', 'Size',\r\n",
        "                'dayofweek_in_cos', 'dayofmonth_in_cos', 'dayofweek_in_sin', 'dayofmonth_in_sin', \r\n",
        "                'dayofweek_out_cos', 'dayofmonth_out_cos', 'dayofweek_out_sin', 'dayofmonth_out_sin'\r\n",
        "          ]\r\n",
        "\r\n",
        "  X_train_duration = data_train.groupby('utrip_id')[feat_names].nth(-1).values\r\n",
        "  X_test_duration = data_test.groupby('utrip_id')[feat_names].nth(-1).values\r\n",
        "\r\n",
        "  X_train_duration = np.hstack([X_train_duration, data_train.groupby('utrip_id')['duration'].sum().values.reshape((-1,1))])\r\n",
        "  X_test_duration = np.hstack([X_test_duration, data_test.groupby('utrip_id')['duration'].sum().values.reshape((-1,1))])\r\n",
        "\r\n",
        "  X_train_affi = data_train.groupby('utrip_id').affiliate_id.last().values\r\n",
        "  X_test_affi = data_test.groupby('utrip_id').affiliate_id.last().values\r\n",
        "\r\n",
        "  X_train_features = pd.get_dummies(data_train.groupby('utrip_id').device_class.last()).values\r\n",
        "  X_test_features = pd.get_dummies(data_test.groupby('utrip_id').device_class.last()).values\r\n",
        "\r\n",
        "  X_train_features2 = pd.get_dummies(data_train.groupby('utrip_id').booker_country.last()).values\r\n",
        "  X_test_features2 = pd.get_dummies(data_test.groupby('utrip_id').booker_country.last()).values\r\n",
        "\r\n",
        "  X_train_features = np.hstack([X_train_features, X_train_features2, X_train_duration, X_train_affi.reshape((-1,1))])\r\n",
        "  X_test_features = np.hstack([X_test_features, X_test_features2, X_test_duration, X_test_affi.reshape((-1,1))])\r\n",
        "\r\n",
        "  return X_train_features, X_test_features\r\n",
        "\r\n",
        "data_train = extract_time(data_train)\r\n",
        "data_test = extract_time(data_test)\r\n",
        "\r\n",
        "X_train_features, X_test_features = get_additionnal_features(data_train, data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0qRYAA-zhq3"
      },
      "source": [
        "def getdata():\r\n",
        "  X_train = data_train.groupby('utrip_id').city_id.apply(lambda x: list(list(x)[:-1])).values\r\n",
        "  X_test = data_test.groupby('utrip_id').city_id.apply(lambda x: list(list(x)[:-1])).values\r\n",
        "\r\n",
        "  X_train_hotel = data_train.groupby('utrip_id').hotel_country.apply(lambda x: list(list(x)[:-1])).values\r\n",
        "  X_test_hotel = data_test.groupby('utrip_id').hotel_country.apply(lambda x: list(list(x)[:-1])).values\r\n",
        "\r\n",
        "  X_train0 = data_train.groupby('utrip_id').city_id.apply(lambda x: list(list(x))).values\r\n",
        "  X_test0 = data_test.groupby('utrip_id').city_id.apply(lambda x: list(list(x))).values\r\n",
        "\r\n",
        "  X_train0_hotel = data_train.groupby('utrip_id').hotel_country.apply(lambda x: list(list(x))).values\r\n",
        "\r\n",
        "  y_train = data_train.groupby('utrip_id').city_id.agg('last').astype(int).values\r\n",
        "  y_test = data_test.groupby('utrip_id').city_id.agg('last').astype(int).values\r\n",
        "\r\n",
        "  y_train_hotel = data_train.groupby('utrip_id').hotel_country.agg('last').astype(int).values\r\n",
        "  y_test_hotel = data_test.groupby('utrip_id').hotel_country.agg('last').astype(int).values\r\n",
        "\r\n",
        "  return X_train, X_test, X_train_hotel, X_test_hotel, X_train0, X_train0_hotel, y_train, y_test, y_train_hotel, y_test_hotel, X_test0\r\n",
        "\r\n",
        "def remove_last_row(df):\r\n",
        "  keep_index = np.concatenate([df.utrip_id.values[:-1]==df.utrip_id.values[1:], np.array([False])])\r\n",
        "  df = df[keep_index]\r\n",
        "  df['Size'] = df.utrip_id.map(df.groupby('utrip_id').user_id.size())\r\n",
        "  df = df[df.Size>1]\r\n",
        "\r\n",
        "  return df\r\n",
        "\r\n",
        "def produce_extra(X_train, X_test, X_train_hotel_extra, X_test_hotel_extra):\r\n",
        "  X_train_len = np.array([len(i) for i in X_train])\r\n",
        "  X_train_extra = X_train[X_train_len>1]\r\n",
        "\r\n",
        "  X_train_hotel_extra_len = np.array([len(i) for i in X_train_hotel_extra])\r\n",
        "  X_train_hotel_extra = X_train_hotel_extra[X_train_hotel_extra_len>1]\r\n",
        "\r\n",
        "  y_train_extra = np.array([int(i[-1]) for i in X_train_extra])\r\n",
        "  y_train_hotel_extra = np.array([int(i[-1]) for i in X_train_hotel_extra])\r\n",
        "\r\n",
        "  X_train_extra = np.array([i[:-1] for i in X_train_extra])\r\n",
        "  X_train_hotel_extra = np.array([i[:-1] for i in X_train_hotel_extra])\r\n",
        "\r\n",
        "  X_test_len = np.array([len(i) for i in X_test])\r\n",
        "  X_test_extra = X_test[X_test_len>1]\r\n",
        "\r\n",
        "  X_test_hotel_extra_len = np.array([len(i) for i in X_test_hotel_extra])\r\n",
        "  X_test_hotel_extra = X_test_hotel_extra[X_test_hotel_extra_len>1]\r\n",
        "\r\n",
        "  y_test_extra = np.array([int(i[-1]) for i in X_test_extra])\r\n",
        "  y_test_hotel_extra = np.array([int(i[-1]) for i in X_test_hotel_extra])\r\n",
        "  X_test_extra = np.array([i[:-1] for i in X_test_extra])\r\n",
        "  X_test_hotel_extra = np.array([i[:-1] for i in X_test_hotel_extra])\r\n",
        "\r\n",
        "  return X_train_extra, y_train_extra, X_test_extra, y_test_extra, y_train_hotel_extra, y_test_hotel_extra, X_train_hotel_extra, X_test_hotel_extra\r\n",
        "\r\n",
        "def partition_arg_topK(matrix, K=10, axis=1):\r\n",
        "    \"\"\"\r\n",
        "    perform topK based on np.argpartition\r\n",
        "    :param matrix: to be sorted\r\n",
        "    :param K: select and sort the top K items\r\n",
        "    :param axis: 0 or 1. dimension to be sorted.\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    a_part = np.argpartition(matrix, K, axis=axis)\r\n",
        "    if axis == 0:\r\n",
        "        row_index = np.arange(matrix.shape[1 - axis])\r\n",
        "        a_sec_argsort_K = np.argsort(matrix[a_part[0:K, :], row_index], axis=axis)\r\n",
        "        return a_part[0:K, :][a_sec_argsort_K, row_index]\r\n",
        "    else:\r\n",
        "        column_index = np.arange(matrix.shape[1 - axis])[:, None]\r\n",
        "        a_sec_argsort_K = np.argsort(matrix[column_index, a_part[:, 0:K]], axis=axis)\r\n",
        "        return a_part[:, 0:K][column_index, a_sec_argsort_K]\r\n",
        "\r\n",
        "\r\n",
        "def make_predictions(model, X, X_feat, proba=False, BATCH_SIZE=2**12):\r\n",
        "  N = X.shape[0]\r\n",
        "  predictions = []\r\n",
        "  for i in range(N//BATCH_SIZE+1):\r\n",
        "    prediction, _ = model.predict((X[i*BATCH_SIZE:(i+1)*BATCH_SIZE], X_feat[i*BATCH_SIZE:(i+1)*BATCH_SIZE]), batch_size=BATCH_SIZE)\r\n",
        "    if proba:\r\n",
        "      predictions.append(prediction)\r\n",
        "    else:\r\n",
        "      label_prediction = partition_arg_topK(-prediction, K=10, axis=1)\r\n",
        "      predictions.append(label_prediction)\r\n",
        "  return np.vstack(predictions)\r\n",
        "\r\n",
        "def make_prediction(kfold=20, embedding_dim=128, random_state=2020):\r\n",
        "\r\n",
        "  # 分割数据集\r\n",
        "  utrip_id_test = data_test.groupby('utrip_id').utrip_id.last().values\r\n",
        "\r\n",
        "  train_user_id = data_train.user_id.unique()\r\n",
        "  kf = KFold(n_splits=kfold, random_state=random_state, shuffle=True)\r\n",
        "\r\n",
        "  user_split = []\r\n",
        "\r\n",
        "  for train_index, test_index in kf.split(np.arange(len(train_user_id))):\r\n",
        "    user_split.append((train_user_id[train_index], train_user_id[test_index]))\r\n",
        "\r\n",
        "  # fold 0\r\n",
        "\r\n",
        "  all_ids = data_train.groupby('utrip_id').user_id.last().values\r\n",
        "\r\n",
        "  fold = 0\r\n",
        "  train_ids = set(list(user_split[fold][0]))\r\n",
        "  train_indexs = np.array(list(map(lambda x: True if x in train_ids else False, all_ids)))\r\n",
        "  valid_indexs = ~train_indexs\r\n",
        "\r\n",
        "  # 训练模型\r\n",
        "  pad_to = 20\r\n",
        "\r\n",
        "\r\n",
        "  BATCH_SIZE = 2**13\r\n",
        "\r\n",
        "  predictions_valid = []\r\n",
        "  predictions_test = []\r\n",
        "\r\n",
        "\r\n",
        "  for fold in range(kfold):\r\n",
        "    start = time.time()\r\n",
        "    print('fold %i'%fold)\r\n",
        "\r\n",
        "    train_ids = set(list(user_split[fold][0]))\r\n",
        "    train_indexs = np.array(list(map(lambda x: True if x in train_ids else False, all_ids)))\r\n",
        "    valid_indexs = ~train_indexs\r\n",
        "\r\n",
        "\r\n",
        "    X_train, X_test, X_train_hotel, X_test_hotel, X_train0, X_train0_hotel, y_train, y_test, y_train_hotel, y_test_hotel, X_test0 = getdata()\r\n",
        "    last_city_test = [int(i[-1]) for i in X_test]\r\n",
        "\r\n",
        "    X_train_extra = X_train.copy()\r\n",
        "    X_test_extra = X_test.copy()\r\n",
        "    X_train_hotel_extra = X_train_hotel.copy()\r\n",
        "    X_test_hotel_extra = X_test_hotel.copy()\r\n",
        "\r\n",
        "    X_train_features, X_test_features = get_additionnal_features(data_train, data_test)\r\n",
        "    X_train_features_extand = X_train_features.copy()\r\n",
        "    X_train_features_extand = X_train_features_extand[train_indexs]\r\n",
        "    X_test_features_extand = X_test_features.copy()\r\n",
        "\r\n",
        "    data_train_extand = data_train.copy()\r\n",
        "    data_test_extand = data_test.copy()\r\n",
        "\r\n",
        "    X_valid = X_train[valid_indexs]\r\n",
        "    X_valid_features = X_train_features[valid_indexs]\r\n",
        "    y_valid = y_train[valid_indexs]\r\n",
        "    y_valid_hotel = y_train_hotel[valid_indexs]\r\n",
        "\r\n",
        "    X_train = X_train[train_indexs]\r\n",
        "    X_train_features = X_train_features[train_indexs]\r\n",
        "    y_train = y_train[train_indexs]\r\n",
        "    y_train_hotel = y_train_hotel[train_indexs]\r\n",
        "\r\n",
        "    X_test_origin = X_test.copy()\r\n",
        "    X_train_origin = X_train.copy()\r\n",
        "    y_train_origin = y_train.copy()\r\n",
        "    y_train_hotel_origin = y_train_hotel.copy()\r\n",
        "\r\n",
        "\r\n",
        "    for i in range(6):\r\n",
        "      print('\\r epoch %s/6 '%(i+1), end='')\r\n",
        "      X_train_extra, y_train_extra, X_test_extra, y_test_extra, y_train_hotel_extra, y_test_hotel_extra, X_train_hotel_extra, X_test_hotel_extra = produce_extra(X_train_extra, X_test_extra, X_train_hotel_extra, X_test_hotel_extra)\r\n",
        "      X_train = np.concatenate([X_train, X_train_extra, X_test_extra, ])\r\n",
        "      y_train = np.concatenate([y_train, y_train_extra, y_test_extra, ])\r\n",
        "      y_train_hotel = np.concatenate([y_train_hotel, y_train_hotel_extra, y_test_hotel_extra, ])\r\n",
        "\r\n",
        "      data_train_extand, data_test_extand = remove_last_row(data_train_extand), remove_last_row(data_test_extand)\r\n",
        "\r\n",
        "      X_train_features_extand_, X_test_features_extand_ = get_additionnal_features(data_train_extand, data_test_extand)\r\n",
        "      X_train_features_extand = np.vstack([X_train_features_extand, X_train_features_extand_, X_test_features_extand_])\r\n",
        "    print('\\r ', end='')\r\n",
        "\r\n",
        "    if fold == 0:\r\n",
        "      if os.path.exists('./word2vec_%s.model'%embedding_dim):\r\n",
        "        model_sg = Word2Vec.load('./word2vec_%s.model'%embedding_dim)\r\n",
        "      else:\r\n",
        "        model_sg = Word2Vec(np.concatenate([X_train0, X_test]), size=embedding_dim, window=1, min_count=1, sg=1, sample=1e-3, negative=30, workers=4, iter=30, seed=1)\r\n",
        "        model_sg.save('./word2vec_%s.model'%embedding_dim)\r\n",
        "      model_emb = model_sg\r\n",
        "\r\n",
        "      embedding_matrix = np.zeros((N_city + 1, embedding_dim))\r\n",
        "      for i in range(N_city):\r\n",
        "        try:\r\n",
        "          embedding_matrix[i+1] = model_emb.wv[str(i)].astype('float32')\r\n",
        "        except:\r\n",
        "          pass\r\n",
        "\r\n",
        "      embedding_layer = Embedding(N_city + 1,\r\n",
        "                      embedding_dim,\r\n",
        "                      weights=[embedding_matrix],\r\n",
        "                      input_length=pad_to,\r\n",
        "                      trainable=False)\r\n",
        "\r\n",
        "    def pad(L, size=embedding_dim):\r\n",
        "      n = len(L)\r\n",
        "      if n<pad_to:\r\n",
        "          L = [0] * (pad_to-n) + [int(i)+1 for i in L]\r\n",
        "      else:\r\n",
        "        L = [int(i)+1 for i in L][-pad_to:]\r\n",
        "      return np.array(L)\r\n",
        "\r\n",
        "\r\n",
        "    X_train = np.asarray(list(map(lambda x: pad(x), (X_train))) ).astype(int)\r\n",
        "    X_valid = np.asarray(list(map(lambda x: pad(x), (X_valid))) ).astype(int)\r\n",
        "\r\n",
        "    X_train_origin = np.asarray(list(map(lambda x: pad(x), (X_train_origin))) ).astype(int)\r\n",
        "    X_test_origin = np.asarray(list(map(lambda x: pad(x), (X_test_origin))) ).astype(int)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def create_model():\r\n",
        "      inputs1 = tf.keras.Input(shape=(None,), dtype=\"int64\")\r\n",
        "      inputs2 = tf.keras.Input(shape=(X_train_features.shape[1]), dtype=\"float32\")\r\n",
        "      \r\n",
        "      input3 = inputs2[:,-1]\r\n",
        "      input3 = layers.Embedding(20000, 24, trainable=True)(input3)\r\n",
        "      \r\n",
        "      input2 = layers.BatchNormalization()(inputs2[:,:-1])\r\n",
        "      input2 = layers.Dense(128, activation='linear')(input2)\r\n",
        "      input2 = layers.BatchNormalization()(input2)\r\n",
        "      input2 = layers.ReLU()(input2)\r\n",
        "      input2 = layers.Dense(64, activation='linear')(input2)\r\n",
        "      input2 = layers.BatchNormalization()(input2)\r\n",
        "      input2 = layers.ReLU()(input2)\r\n",
        "\r\n",
        "      input2 = layers.concatenate([input3, input2])\r\n",
        "\r\n",
        "\r\n",
        "      masked_inputs1 = embedding_layer(inputs1)\r\n",
        "      masked_inputs1 = layers.LayerNormalization()(masked_inputs1)\r\n",
        "\r\n",
        "      masked_inputs1 = layers.SpatialDropout1D(0.1)(masked_inputs1)\r\n",
        "      x_a = layers.Bidirectional(\r\n",
        "        layers.LSTM(128, return_sequences=True, recurrent_initializer=\"orthogonal\", kernel_initializer=\"glorot_normal\",\r\n",
        "            ))(masked_inputs1)\r\n",
        "      x_a = layers.LayerNormalization()(x_a)\r\n",
        "      x_a = layers.SpatialDropout1D(0.1)(x_a)\r\n",
        "      x_a = layers.Bidirectional(\r\n",
        "        layers.GRU(128, return_sequences=False, recurrent_initializer=\"orthogonal\", kernel_initializer=\"glorot_normal\",\r\n",
        "            ))(x_a)\r\n",
        "      x_a = layers.LayerNormalization()(x_a)\r\n",
        "\r\n",
        "\r\n",
        "      x_t = masked_inputs1[:,-1,:]\r\n",
        "\r\n",
        "      x_a = layers.Dropout(0.1)(x_a)\r\n",
        "      x_a = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(3e-5), kernel_initializer='uniform')(x_a)\r\n",
        "      x_a = layers.LayerNormalization()(x_a)\r\n",
        "\r\n",
        "      x_t = layers.Dropout(0.1)(x_t)\r\n",
        "      x_t = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(3e-5), kernel_initializer='uniform')(x_t)\r\n",
        "      x_t = layers.LayerNormalization()(x_t)\r\n",
        "      \r\n",
        "      x_ = layers.concatenate([x_a, x_t])\r\n",
        "      x_ = layers.Dropout(0.1)(x_)\r\n",
        "      alpha = layers.Dense(512, activation='sigmoid')(x_)\r\n",
        "\r\n",
        "      x = alpha*x_a + (1-alpha)*x_t\r\n",
        "      x = layers.LayerNormalization()(x)\r\n",
        "      x = layers.concatenate([input2, x])\r\n",
        "      x = layers.Dropout(0.5)(x)\r\n",
        "\r\n",
        "\r\n",
        "      x2 = Dense(N_hotel, activation='softmax', name='country')(x)\r\n",
        "      x = Dense(N_city, activation='softmax', name='city', kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\r\n",
        "\r\n",
        "      model = keras.Model(inputs=[inputs1, inputs2], outputs=[x, x2])\r\n",
        "      return model\r\n",
        "\r\n",
        "    model = create_model()\r\n",
        "    if os.path.exists('fold_%s_%s.h5'%(fold, embedding_dim)):\r\n",
        "      model.load_weights('fold_%s_%s.h5'%(fold, embedding_dim))\r\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "      optimizer = AdaBeliefOptimizer(learning_rate=1e-3, epsilon=1e-14, rectify=False, print_change_log = False)\r\n",
        "      model.compile(optimizer=optimizer, # tf.keras.optimizers.Adam(),\r\n",
        "                        loss=loss,\r\n",
        "                        loss_weights=[1, 0.3, 0.3],\r\n",
        "                        metrics=['acc',tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)],\r\n",
        "                        )\r\n",
        "      model.evaluate((X_valid, X_valid_features), (y_valid, y_valid_hotel))\r\n",
        "    else:\r\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "      rlr = ReduceLROnPlateau(monitor = 'val_city_sparse_top_k_categorical_accuracy', factor = 0.3, patience = 3, verbose = 1, \r\n",
        "                              min_delta = 1e-4, mode = 'max', min_lr=1e-5)\r\n",
        "      \r\n",
        "      ckp_path = 'fold_%s_%s.h5'%(fold, embedding_dim)\r\n",
        "      ckp = ModelCheckpoint(ckp_path, monitor = 'city_sparse_top_k_categorical_accuracy', verbose = 0, \r\n",
        "                      save_best_only = True, save_weights_only = True, mode = 'max')\r\n",
        "      optimizer = AdaBeliefOptimizer(learning_rate=1e-3, epsilon=1e-14, rectify=False, print_change_log = False)\r\n",
        "      model.compile(optimizer=optimizer, # tf.keras.optimizers.Adam(),\r\n",
        "                        loss=loss,\r\n",
        "                        loss_weights=[1, 0.3, 0.3],\r\n",
        "                        metrics=['acc',tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)],\r\n",
        "                        \r\n",
        "                        )\r\n",
        "\r\n",
        "      history = model.fit((X_train, X_train_features_extand), (y_train, y_train_hotel), \r\n",
        "                          epochs=40, \r\n",
        "                          validation_data=((X_valid, X_valid_features), (y_valid, y_valid_hotel)),\r\n",
        "                          shuffle=True,\r\n",
        "                          batch_size=BATCH_SIZE,\r\n",
        "                          callbacks = [rlr,ckp],\r\n",
        "                          verbose=0\r\n",
        "                          )\r\n",
        "      # model.save_weights('fold_%s_%s.h5'%(fold, embedding_dim))\r\n",
        "      model.load_weights('fold_%s_%s.h5'%(fold, embedding_dim))\r\n",
        "\r\n",
        "      \r\n",
        "      optimizer = AdaBeliefOptimizer(learning_rate=3e-5, epsilon=1e-14, rectify=False, print_change_log = False)\r\n",
        "      model.compile(optimizer=optimizer, # tf.keras.optimizers.Adam(),\r\n",
        "                        loss=loss,\r\n",
        "                        loss_weights=[1, 0.3, 0.3],\r\n",
        "                        metrics=['acc',tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)],\r\n",
        "                        )\r\n",
        "      ckp_path = 'fold_%s_%s.h5'%(fold, embedding_dim)\r\n",
        "      ckp = ModelCheckpoint(ckp_path, monitor = 'city_sparse_top_k_categorical_accuracy', verbose = 0, \r\n",
        "                      save_best_only = True, save_weights_only = True, mode = 'max')\r\n",
        "      rlr = ReduceLROnPlateau(monitor = 'val_city_sparse_top_k_categorical_accuracy', factor = 0.3, patience = 3, verbose = 1, \r\n",
        "                              min_delta = 1e-4, mode = 'max', min_lr=1e-6)\r\n",
        "      history = model.fit((X_train_origin, X_train_features), (y_train_origin, y_train_hotel_origin, ), \r\n",
        "                          epochs=40, \r\n",
        "                          validation_data=((X_valid, X_valid_features), (y_valid, y_valid_hotel)),\r\n",
        "                          shuffle=True,\r\n",
        "                          batch_size=BATCH_SIZE,\r\n",
        "                          callbacks = [rlr, ckp],\r\n",
        "                          verbose=0\r\n",
        "                          )\r\n",
        "      # model.save_weights('fold_%s_%s.h5'%(fold, embedding_dim))\r\n",
        "      model.evaluate((X_valid, X_valid_features), (y_valid, y_valid_hotel))\r\n",
        "\r\n",
        "    # prediction_valid = make_predictions(model, X_valid, X_valid_features, True)\r\n",
        "    prediction_test = make_predictions(model, X_test_origin, X_test_features, False)\r\n",
        "\r\n",
        "    # predictions_valid.append(prediction_valid)\r\n",
        "    predictions_test.append(prediction_test)\r\n",
        "\r\n",
        "    print(time.time()-start)\r\n",
        "\r\n",
        "  def inverse_trans2(pred):\r\n",
        "    shape = pred.shape\r\n",
        "    res = le.inverse_transform(pred.reshape(-1))\r\n",
        "\r\n",
        "    return res.reshape(shape)\r\n",
        "\r\n",
        "  np.save(path+'/'+'preds_%sfold_%s.npy'%(fold, embedding_dim), np.array(predictions_test))\r\n",
        "  predictions_test_trans = [inverse_trans2(pred) for pred in predictions_test]\r\n",
        "  np.save(path+'/'+'preds_%sfold_%s_trans.npy'%(fold, embedding_dim), np.array(predictions_test_trans))\r\n",
        "  all_preds = np.hstack(predictions_test_trans)\r\n",
        "\r\n",
        "  return predictions_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLi7KQsQiZQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ee542e-3d05-4548-e08d-799b50b7eee3"
      },
      "source": [
        "predictions_test_128 = make_prediction(kfold=20, embedding_dim=224, random_state=2020)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 0\n",
            " \n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "342/342 [==============================] - 5s 13ms/step - loss: 3.7850 - city_loss: 3.4150 - country_loss: 0.4368 - city_acc: 0.3466 - city_sparse_top_k_categorical_accuracy: 0.5756 - country_acc: 0.8809 - country_sparse_top_k_categorical_accuracy: 0.9748\n",
            "2246.9068315029144\n",
            "fold 1\n",
            " \n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "342/342 [==============================] - 5s 13ms/step - loss: 3.8812 - city_loss: 3.5075 - country_loss: 0.4578 - city_acc: 0.3403 - city_sparse_top_k_categorical_accuracy: 0.5702 - country_acc: 0.8761 - country_sparse_top_k_categorical_accuracy: 0.9762\n",
            "2052.3867177963257\n",
            "fold 2\n",
            " \n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "341/341 [==============================] - 5s 14ms/step - loss: 3.8348 - city_loss: 3.4449 - country_loss: 0.4537 - city_acc: 0.3430 - city_sparse_top_k_categorical_accuracy: 0.5708 - country_acc: 0.8760 - country_sparse_top_k_categorical_accuracy: 0.9746\n",
            "2038.3598074913025\n",
            "fold 3\n",
            " \n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "340/340 [==============================] - 5s 13ms/step - loss: 3.8072 - city_loss: 3.4560 - country_loss: 0.4283 - city_acc: 0.3514 - city_sparse_top_k_categorical_accuracy: 0.5789 - country_acc: 0.8820 - country_sparse_top_k_categorical_accuracy: 0.9764\n",
            "2047.7977585792542\n",
            "fold 4\n",
            " \n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "340/340 [==============================] - 5s 13ms/step - loss: 3.8549 - city_loss: 3.4769 - country_loss: 0.4525 - city_acc: 0.3468 - city_sparse_top_k_categorical_accuracy: 0.5745 - country_acc: 0.8756 - country_sparse_top_k_categorical_accuracy: 0.9770\n",
            "2035.4067447185516\n",
            "fold 5\n",
            " \n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "340/340 [==============================] - 5s 13ms/step - loss: 3.8110 - city_loss: 3.4360 - country_loss: 0.4561 - city_acc: 0.3415 - city_sparse_top_k_categorical_accuracy: 0.5759 - country_acc: 0.8732 - country_sparse_top_k_categorical_accuracy: 0.9755\n",
            "2060.919280767441\n",
            "fold 6\n",
            " \n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "340/340 [==============================] - 5s 14ms/step - loss: 3.7822 - city_loss: 3.4078 - country_loss: 0.4436 - city_acc: 0.3549 - city_sparse_top_k_categorical_accuracy: 0.5824 - country_acc: 0.8817 - country_sparse_top_k_categorical_accuracy: 0.9761\n",
            "2049.9950881004333\n",
            "fold 7\n",
            " \n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 3.8277 - city_loss: 3.4650 - country_loss: 0.4435 - city_acc: 0.3466 - city_sparse_top_k_categorical_accuracy: 0.5820 - country_acc: 0.8775 - country_sparse_top_k_categorical_accuracy: 0.9765\n",
            "2068.3410573005676\n",
            "fold 8\n",
            " \n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "339/339 [==============================] - 4s 13ms/step - loss: 3.8094 - city_loss: 3.4517 - country_loss: 0.4502 - city_acc: 0.3485 - city_sparse_top_k_categorical_accuracy: 0.5819 - country_acc: 0.8788 - country_sparse_top_k_categorical_accuracy: 0.9749\n",
            "2032.8483278751373\n",
            "fold 9\n",
            " \n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "342/342 [==============================] - 5s 13ms/step - loss: 3.8101 - city_loss: 3.4632 - country_loss: 0.4329 - city_acc: 0.3461 - city_sparse_top_k_categorical_accuracy: 0.5765 - country_acc: 0.8832 - country_sparse_top_k_categorical_accuracy: 0.9778\n",
            "2043.5986349582672\n",
            "fold 10\n",
            " \n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "343/343 [==============================] - 5s 13ms/step - loss: 3.8043 - city_loss: 3.4252 - country_loss: 0.4427 - city_acc: 0.3482 - city_sparse_top_k_categorical_accuracy: 0.5751 - country_acc: 0.8781 - country_sparse_top_k_categorical_accuracy: 0.9775\n",
            "2037.0792698860168\n",
            "fold 11\n",
            " \n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "341/341 [==============================] - 5s 14ms/step - loss: 3.8172 - city_loss: 3.4655 - country_loss: 0.4475 - city_acc: 0.3379 - city_sparse_top_k_categorical_accuracy: 0.5716 - country_acc: 0.8745 - country_sparse_top_k_categorical_accuracy: 0.9766\n",
            "2048.134940624237\n",
            "fold 12\n",
            " \n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "341/341 [==============================] - 5s 14ms/step - loss: 3.8106 - city_loss: 3.4339 - country_loss: 0.4321 - city_acc: 0.3472 - city_sparse_top_k_categorical_accuracy: 0.5750 - country_acc: 0.8843 - country_sparse_top_k_categorical_accuracy: 0.9768\n",
            "2046.8675832748413\n",
            "fold 13\n",
            " \n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "338/338 [==============================] - 5s 13ms/step - loss: 3.8720 - city_loss: 3.5065 - country_loss: 0.4549 - city_acc: 0.3435 - city_sparse_top_k_categorical_accuracy: 0.5715 - country_acc: 0.8740 - country_sparse_top_k_categorical_accuracy: 0.9762\n",
            "2051.622709274292\n",
            "fold 14\n",
            " \n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzRKd1u_bPYz"
      },
      "source": [
        "def inverse_trans2(pred):\r\n",
        "  shape = pred.shape\r\n",
        "  res = le.inverse_transform(pred.reshape(-1))\r\n",
        "\r\n",
        "  return res.reshape(shape)\r\n",
        "\r\n",
        "np.save('preds_20fold_128.npy', np.array(predictions_test))\r\n",
        "predictions_test_trains = [inverse_trans2(pred) for pred in predictions_test]\r\n",
        "np.save('preds_20fold_128_trans.npy', np.array(predictions_test_trains))\r\n",
        "all_preds = np.hstack(predictions_test_trains)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "1fa026a61c844c4a93d010c9bde302b7",
            "d8f42e827ed641909fb89abb1b2b7ba2",
            "8f1b98b8d4bd4b77b020bc28dce19dbc",
            "7b4c1103e511446aa61d2bf23cd81dc1",
            "8b789b42b84c4f49b8437d7e0741376f",
            "8ff944d0ce084b4d9f5f0c03b177dd1e",
            "ed5b78921aa3459098367fdfccadc40f",
            "a567f2172d254ef49b6b2bddb93ffe98"
          ]
        },
        "id": "6TV3W7wKuRUj",
        "outputId": "ddaec312-46b8-400b-e96a-8d1aa19bf88e"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "\r\n",
        "final_pred = []\r\n",
        "for index in tqdm(range(len(all_preds))):\r\n",
        "  dic = defaultdict(int)\r\n",
        "  for i,pred in enumerate(all_preds[index]):\r\n",
        "    dic[pred] += 10-(i%10)\r\n",
        "  preds = [item[0] for item in sorted(dic.items(), key=lambda item: -item[1])[:4]]\r\n",
        "  final_pred.append(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fa026a61c844c4a93d010c9bde302b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70662.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAOfXNHiuJEo"
      },
      "source": [
        "final_pred = np.array(final_pred)\r\n",
        "submission = pd.DataFrame({'utrip_id': data_test.utrip_id.unique(),\r\n",
        "                           'city_id_1':final_pred[:,0],\r\n",
        "                           'city_id_2':final_pred[:,1],\r\n",
        "                           'city_id_3':final_pred[:,2],\r\n",
        "                           'city_id_4':final_pred[:,3]\r\n",
        "                           })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PU4jXNiRrIYG",
        "outputId": "887fd9e8-16a6-4379-ffaa-09de58023042"
      },
      "source": [
        "submission.to_csv('./submission_team.csv', index=None)\r\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>utrip_id</th>\n",
              "      <th>city_id_1</th>\n",
              "      <th>city_id_2</th>\n",
              "      <th>city_id_3</th>\n",
              "      <th>city_id_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000066_2</td>\n",
              "      <td>3809</td>\n",
              "      <td>13471</td>\n",
              "      <td>46854</td>\n",
              "      <td>20931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000270_1</td>\n",
              "      <td>29207</td>\n",
              "      <td>22175</td>\n",
              "      <td>13931</td>\n",
              "      <td>53636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000441_1</td>\n",
              "      <td>47759</td>\n",
              "      <td>35160</td>\n",
              "      <td>13260</td>\n",
              "      <td>9485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100048_1</td>\n",
              "      <td>13530</td>\n",
              "      <td>57266</td>\n",
              "      <td>26235</td>\n",
              "      <td>45030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000543_1</td>\n",
              "      <td>52818</td>\n",
              "      <td>3145</td>\n",
              "      <td>25473</td>\n",
              "      <td>26951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    utrip_id  city_id_1  city_id_2  city_id_3  city_id_4\n",
              "0  1000066_2       3809      13471      46854      20931\n",
              "1  1000270_1      29207      22175      13931      53636\n",
              "2  1000441_1      47759      35160      13260       9485\n",
              "3   100048_1      13530      57266      26235      45030\n",
              "4  1000543_1      52818       3145      25473      26951"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6Qgm1F0Xxg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqlAoZ4ZsEtc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOw5suUzLN0c"
      },
      "source": [
        "def naive_arg_topK(matrix, K=4, axis=1):\r\n",
        "    \"\"\"\r\n",
        "    perform topK based on np.argsort\r\n",
        "    :param matrix: to be sorted\r\n",
        "    :param K: select and sort the top K items\r\n",
        "    :param axis: dimension to be sorted.\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    full_sort = np.argsort(matrix, axis=axis)\r\n",
        "    return full_sort.take(np.arange(K), axis=axis)\r\n",
        "\r\n",
        "def partition_arg_topK(matrix, K=4, axis=1):\r\n",
        "    \"\"\"\r\n",
        "    perform topK based on np.argpartition\r\n",
        "    :param matrix: to be sorted\r\n",
        "    :param K: select and sort the top K items\r\n",
        "    :param axis: 0 or 1. dimension to be sorted.\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    a_part = np.argpartition(matrix, K, axis=axis)\r\n",
        "    if axis == 0:\r\n",
        "        row_index = np.arange(matrix.shape[1 - axis])\r\n",
        "        a_sec_argsort_K = np.argsort(matrix[a_part[0:K, :], row_index], axis=axis)\r\n",
        "        return a_part[0:K, :][a_sec_argsort_K, row_index]\r\n",
        "    else:\r\n",
        "        column_index = np.arange(matrix.shape[1 - axis])[:, None]\r\n",
        "        a_sec_argsort_K = np.argsort(matrix[column_index, a_part[:, 0:K]], axis=axis)\r\n",
        "        return a_part[:, 0:K][column_index, a_sec_argsort_K]\r\n",
        "\r\n",
        "\r\n",
        "partition_arg_topK(prediction[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkrqSKjSLNqt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxI2RhIkKQcg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iI5XYsErBAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvz-7QSwLd8v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSFmebVMzhin"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}